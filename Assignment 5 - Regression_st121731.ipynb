{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Romen Samuel Wabina ST121731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load boston setting X as boston.data and y as boston.target\n",
    "\n",
    "Attempt the grid search using polyregression + (linear, ridge, lasso, elastic net), and \n",
    "\n",
    "Does feature mechanisms on ridge/lasso/elastic helps here?\n",
    "\n",
    "what is the optimal polynomial degree?  What does it mean?\n",
    "\n",
    "why do you think the result is like this?\n",
    "\n",
    "what is the value of lambdas, and what does it means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load boston dataset from sklearn here\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "# boston.feature_names\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X, y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
      "                ('linearregression', LinearRegression(normalize=True))])\n",
      "Best params:  {'polynomialfeatures__degree': 1}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b6e89f9d72e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m#Estimate the accuracy of the model using different  statistical tests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Coefficients: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"r^2 = {r2_score(y_test, y_pred):.3f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"MSE = {mean_squared_error(y_test, y_pred):.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_name' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X, y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "#Attempt the grid search using polyregression + \n",
    "#(linear, ridge, lasso, elastic net)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model_names = ['Linear Regression', 'Ridge Regression', \n",
    "               'Lasso Regression', 'Elastic Net Regression']\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#Initialize regression models in the make_pipeline function\n",
    "linear = make_pipeline(PolynomialFeatures(), LinearRegression(normalize = True))\n",
    "ridge = make_pipeline(PolynomialFeatures(), Ridge(normalize = True, tol = 0.01))\n",
    "lasso = make_pipeline(PolynomialFeatures(), Lasso(normalize = True, tol = 0.01))\n",
    "elastic_net = make_pipeline(PolynomialFeatures(), ElasticNet(normalize = True, tol = 0.01))\n",
    "models = [linear, ridge, lasso, elastic_net]\n",
    "#Polynomial degrees and lambdas\n",
    "alphas = np.logspace(-1, -4, 4)\n",
    "degrees = np.arange(1, 10)\n",
    "#List of parameters\n",
    "linear_params = {'polynomialfeatures__degree': degrees}\n",
    "ridge_params = {'polynomialfeatures__degree': degrees,\n",
    "                'ridge__alpha': alphas}\n",
    "lasso_params = {'polynomialfeatures__degree': degrees,\n",
    "                'lasso__alpha': alphas}\n",
    "elasticnet_params = {'polynomialfeatures__degree': degrees,\n",
    "                    'elasticnet__alpha': alphas}\n",
    "parameters = [linear_params, ridge_params, lasso_params, elasticnet_params]\n",
    "#Import relevant libraries for regression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for ix, model in enumerate(models):\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state=42)\n",
    "    print(model)\n",
    "    grid = GridSearchCV(models[ix], parameters[ix], cv=cv)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    #Finding the best parameters\n",
    "    print(\"Best params: \", grid.best_params_)\n",
    "\n",
    "    #Predicions\n",
    "    model = grid.best_estimator_\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Estimate the accuracy of the model using different  statistical tests\n",
    "    print(\"Coefficients: \", model.named_steps[features_name[ix]].coef_)\n",
    "    print(f\"r^2 = {r2_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"MSE = {mean_squared_error(y_test, y_pred):.2f}\")\n",
    "    n, p = X.shape[0], X.shape[1]\n",
    "    adjusted_rsqrt = 1-(1-r2_score(y_test, y_pred))*(n-1)/(n-p-1)\n",
    "    print(f\"adjusted $r^2$ = {adjusted_rsqrt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly evident that Lasso and Ridge Regression provides better accuracy compared to Elastic Net Regression. This was compared using the coefficient of determination and its adjusted $R^{2}$. In this case of polynomial regression, adjusted $R^{2}$ can be an efficient measure of comparison among the models since $R^{2}$ affects the linear relationship of $X$ and $y$.\n",
    "\n",
    "Our objective function in regression involves minimizing the mean squared errors. Among the models simulated above, Ridge Regression provdes better accuracy since it is closer to $0$. However, $MSE$ heavily relies on variance which is prone to existence of outliers. This makes the dataset having a non-parametric dstribution. \n",
    "\n",
    "In terms of coefficients, one observation we can always rely on is its linear relationship of $X$ given $y$. In this example, the Lasso and Ridge regression models produce more negative coefficients. That is, indicates a negative linear relationship to its datasets. \n",
    "\n",
    "We provided a lot if $\\beta$ coefficients in three regression models. Having too many predictors, fitting the model without pernalization will result in large prediction intervals. Hence, the least square regression estimator may not uniqely exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#only 10 out of 100 features are informative, add some noise to add to the difficulty of the problem\n",
    "X, y, coef = make_regression(n_samples = 1000, n_features = 100, coef=True,\n",
    "                         random_state=42, bias=6, noise=50, n_informative=10)\n",
    "\n",
    "#scaling does not really help with simple linear regression\n",
    "#since the coefficients can be multiply to certain order but\n",
    "#with same result.  However, regularized models will be\n",
    "#affected.  The idea is that the constraint is applied to the\n",
    "#sum of a function of coefficients.  If we inflate an attribute,\n",
    "#the coefficient will be deflated, which will affect\n",
    "#the penalization.  Thus it is best to scale for all regression\n",
    "#problems since it does not hurt\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#perform Ridge regression\n",
    "#plot training and validation errors as function of lambda\n",
    "#also plot coefficients and coefficients error as function of lambda\n",
    "#coefficients can be obtained simply using model.coef_\n",
    "#coeffcient error can be computed using mean_squared_error(model.coef_, w)\n",
    "#interpret what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Perform Ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "parameters = np.logspace(-10, 10, 100)\n",
    "\n",
    "#training and validation errors \n",
    "#coefficients and coefficients error as function of lambda\n",
    "training_errors = []\n",
    "validation_errors = []\n",
    "coefficients = []\n",
    "coefficients_errors = []\n",
    "\n",
    "#metrics.SCORERS.keys()\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for parameter in parameters:\n",
    "    \n",
    "    #Perform Ridge regression for every parameter\n",
    "    ridge = Ridge(alpha = parameter, tol = 0.01, normalize = True)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    #Coefficients and its errors\n",
    "    coefficients.append(ridge.coef_)\n",
    "    coefficients_mse = mean_squared_error(ridge.coef_, coef)\n",
    "    coefficients_errors.append(coefficients_mse)\n",
    "    \n",
    "    #Making predictions using training set\n",
    "    #Also finding its training errors to check whether underfit, overfit, or goodfit\n",
    "    train_predictions = ridge.predict(X_train)\n",
    "    train_mse = mean_squared_error(y_train, train_predictions)\n",
    "    training_errors.append(train_mse)\n",
    "    \n",
    "    #Making predictions using testing set\n",
    "    #Especially for unseen datasets\n",
    "    #Also determining validation errors to check its fitting\n",
    "    test_predictions = ridge.predict(X_test)\n",
    "    test_mse = mean_squared_error(y_test, test_predictions)\n",
    "    validation_errors.append(test_mse)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 3, figsize = (16, 4))\n",
    "\n",
    "#plot training and validation errors as function of lambda\n",
    "ax[0].plot(parameters, training_errors, label = 'Training Errors')\n",
    "ax[0].plot(parameters, validation_errors, label = 'Validation Errors')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('Parameters')\n",
    "ax[0].set_ylabel('Errors')\n",
    "ax[0].legend(ncol = 1, loc = 'best')\n",
    "#also plot coefficients and coefficients error as function of lambda\n",
    "\n",
    "ax[1].plot(parameters, coefficients)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Parameters')\n",
    "ax[1].set_ylabel('Coefficients')\n",
    "\n",
    "ax[2].plot(parameters, coefficients_errors)\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].set_xlabel('Parameters')\n",
    "ax[2].set_ylabel('Coefficient Errors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training errors determine how well it has learned using the training data. This logic same goes with the testing data set, that is validation errors. However, the validation error manifests the generalization of the learned model, especially how it fits to the data that it has not been trained on. In the plot above, training errors are larger than the validation errosrs especially with parameters ranges from $10^0$ to $10^{10}$. As the polynomial degree increases, the training error continually increases due to decreased flexibility. The validation error only decreases as we provide an increasing polynomial degree. Hence, we can say that the model provides good fit. \n",
    "\n",
    "The second plot shows Ridge Trace between the standardized $\\beta$ versus $\\theta$. The estimates stabilize at approximately higher than $10^1$ parameters. However, once $\\lambda$ is getting larger, values of $\\beta$ converges to zero. The plot also shows that $\\beta$ at the left part of $10^1$ suffers from underfitting since $\\beta$ are too small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "implement Linear regression from scratch using \n",
    "\n",
    "- the closed form OLS\n",
    "\n",
    "- the gradient descent\n",
    "\n",
    "- (optional challenge) the closed form OLS using pseudo inverse\n",
    "\n",
    "- LinearRegression() by sklearn. - please research which algorithm that sklearn use?\n",
    "\n",
    "measure which one is faster, try to vary the n_features\n",
    "\n",
    "so what do you think, closed_form or gradient descent?\n",
    "\n",
    "use the same X y from #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libarary to check running time of the code\n",
    "from time import time\n",
    "\n",
    "num_features = np.linspace(1, X.shape[1], 5, dtype = int)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1) \n",
    "\n",
    "#Closed form OLS\n",
    "print('=================Closed Form OLS=================')\n",
    "from numpy.linalg import lstsq\n",
    "def normaleqs_LinearRegression():\n",
    "    start = time()\n",
    "    normaleqs_time = []\n",
    "    for feature in num_features:\n",
    "        inverse = X_train[:,:feature].T.dot(X_train[:,:feature])\n",
    "        weights = inverse.dot(X_train[:,:feature].T).dot(y_train)\n",
    "        normaleqs_predictions = X_test[:,:feature] @ weights\n",
    "        print(f'Results finish using {feature} features with MSE of ' + \n",
    "              f'{mean_squared_error(y_test, normaleqs_predictions)}' +\n",
    "              f'\\nusing time of {time()-start}')\n",
    "        #normaleqs_time.append(time()-start)\n",
    "    #print(normaleqs_time)\n",
    "normaleqs_LinearRegression()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.linalg import inv\n",
    "\n",
    "print('\\n=================Gradient Descent=================')\n",
    "test_num_feature = np.linspace(1, X.shape[1], 5, dtype=int)\n",
    "def gradient_descent_LinearReg(X, y, num_feature, max_iter=10, tol=0.0001):\n",
    "    X = X[:, :num_feature]\n",
    "    w = np.zeros(X.shape[1])\n",
    "    m = len(y)\n",
    "    l_rate = 0.0001 \n",
    "    for i in range(max_iter):\n",
    "        pred = X @ w\n",
    "        if (mean_squared_error(y, pred) < tol):\n",
    "            break\n",
    "        error = pred - y\n",
    "        w = w - ((l_rate * 1/m) * np.dot(X.T, error))\n",
    "    return w, i\n",
    "for num_feature in test_num_feature: \n",
    "    start = time()\n",
    "    w, i = gradient_descent_LinearReg(X_train, y_train, num_feature=num_feature, max_iter=2000)\n",
    "    pred = X_test[:, :num_feature] @ w\n",
    "    print(f\"Results finish using {i+1} iteration and {num_feature} features with MSE of \" +\n",
    "    f\"{mean_squared_error(y_test, pred)}\" + \n",
    "        f\" using time of {time() - start}\")\n",
    "    \n",
    "print('\\n===============Closed Form OLS using Pseudo Inverse=================') \n",
    "from numpy.linalg import inv\n",
    "#Linear regression has a function pseudo to set the regression model under pseudo-inverse function\n",
    "def linear_regression_ols(X, y, num_feature, pseudo = False):\n",
    "    X = X[:, :feature]\n",
    "    w = np.dot(np.linalg.pinv(X), y)\n",
    "    return w\n",
    "\n",
    "for feature in test_num_feature:\n",
    "    start = time()\n",
    "    w = linear_regression_ols(X_train, y_train, num_feature = feature, pseudo=True)\n",
    "    pred = X_test[:, :feature] @ w\n",
    "    print(f\"Results finish using {num_feature} features with MSE of \" +\n",
    "    f\"{mean_squared_error(y_test, pred)}\" + \n",
    "        f\" using time of {time() - start}\")\n",
    "             \n",
    "print('\\n===============sklearn Linear Regression=================') \n",
    "#Linear Regression using sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from time import time\n",
    "def sklearn_LinearRegression():\n",
    "    start = time()\n",
    "    sklearnLR_time = []\n",
    "    for feature in num_features:\n",
    "        linear = LinearRegression()\n",
    "        linear.fit(X_train[: , :feature], y_train)\n",
    "        sklearn_predictions = linear.predict(X_test[:,: feature])\n",
    "        sklearnLR_time.append(time() - start)\n",
    "    print(sklearnLR_time)\n",
    "normaleqs_LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of its usage, Gradient Descent proves to run slower compared to other algorithms of regression models. Its running time may be dependent to the alpha we used in the simulation above. Lower alphas can provide better accuracy to its model. HOwever, one advantage to this is that it will provide a very slow running time. The OLS performed faster among the models shown from above. Normal equations for the closed form OLS will solve directly the coefficients and parameters of the Linear Regression model. In this way, running time of OLS is faster. \n",
    "\n",
    "In terms of the accuracy, while Gradient Descent runs slowest, there are no sufficient evidence to say that Gradient Descent does not perform very well. However, if you look very closely between the Gradient Descent against Pseudo-inverse, they almost had the same Mean Squared Error. But Pseudo-inverse runs faster than the Gradient Descent since it only requires X and its inverse from (X.TX)-1 X.T. But the downsize to this is that there is no guarantee that every regression problem, pseudo-inverse will exist. It only exists when (X.TX)-1 is non-singulat, and in this case, the Beta is unique.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "implement Ridge regression from scratch using \n",
    "\n",
    " - the closed form OLS\n",
    "\n",
    " - stochastic gradient descent\n",
    "\n",
    " - SGDRegressor() of sklearn using penalty as l2\n",
    "\n",
    " - Ridge() by sklearn\n",
    "\n",
    "loop through several lambda and print the MSE\n",
    "\n",
    "compare the time as well\n",
    "\n",
    "use the same X y from #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for the closed form OLS\n",
    "Given the same features and parameters, a ridge regressor for some parameter value  Î²  is one which minimizes: $\\text{min}_\\beta \\Big\\{ (y - X\\beta)^T (y-X\\beta) + \\lambda \\beta^T \\beta \\Big\\}$. The closed form solution for ridge regression is:\n",
    "$$ \\text{min}_\\beta \\Big\\{ (y - X\\beta)^T (y-X\\beta) + \\lambda \\beta^T \\beta \\Big\\} \\Leftrightarrow (X^T X + \\lambda I)\\beta = X^T y $$\n",
    "Hence, the normal equations for Ridge Regression is $$ (X^T X + \\lambda I)\\beta = X^T y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closed form OLS\n",
    "#Ridge regression\n",
    "\n",
    "normaleqs_training_errors = []\n",
    "normaleqs_validation_errors = []\n",
    "normaleqs_duration = []\n",
    "\n",
    "def normaleqs_ridge():\n",
    "    start = time()\n",
    "    alphas = np.logspace(-10, 10, 10)\n",
    "    for alpha in alphas:\n",
    "        train_iden_matrix = np.eye(X_train.shape[1])\n",
    "        #Using the normal equations for Ridge Regression as stated above\n",
    "        weights_train = np.linalg.solve((X_train.T @ X_train) + alpha * train_iden_matrix, \n",
    "                              X_train.T @ y_train)\n",
    "        train_predictions_normaleqs = X_train @ weights_train\n",
    "        train_normaleqs_mse =mean_squared_error(y_train, train_predictions_normaleqs)\n",
    "        normaleqs_training_errors.append(train_normaleqs_mse)\n",
    "        \n",
    "        \n",
    "        test_iden_matrix = np.eye(X_test.shape[1])\n",
    "        weights_test = np.linalg.solve((X_test.T @ X_test) + alpha * test_iden_matrix, \n",
    "                              X_test.T @ y_test)\n",
    "        test_predictions_normaleqs = X_test @ weights_test\n",
    "        test_normaleqs_mse = mean_squared_error(y_test, test_predictions_normaleqs)\n",
    "        normaleqs_validation_errors.append(test_normaleqs_mse)\n",
    "        \n",
    "        normaleqs_duration.append(time() - start)\n",
    "    return normaleqs_training_errors, normaleqs_validation_errors, normaleqs_duration\n",
    "        #print(f'Using alpha of {alpha}, Normal Equations has MSE of {normaleqs_mse} with {duration} second execution time.')\n",
    "#normaleqs_ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent\n",
    "ridgeSGD_training_errors = []\n",
    "ridgeSGD_validation_errors = []\n",
    "ridgeSGD_duration = []\n",
    "\n",
    "def ridge_sgd():\n",
    "    start = time()\n",
    "    max_iter = 1000\n",
    "    tol = 0.001\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    m = len(y_train)  \n",
    "    l_rate = 0.01\n",
    "    start = time()\n",
    "    alphas = np.logspace(-10, 10, 10)\n",
    "    \n",
    "    \n",
    "    for lamb in alphas:\n",
    "        for i in range(max_iter):\n",
    "            for j in range(X_train.shape[0]):  \n",
    "                random_row_to_pick = np.random.randint(0, X_train.shape[0])\n",
    "                X_i = X_train[random_row_to_pick, :].reshape(1, X_train.shape[1])\n",
    "                y_i = y_train[[random_row_to_pick]] #shape (n, )\n",
    "                train_predictions_normaleqs = X_i @ w\n",
    "                error = train_predictions_normaleqs - y_i\n",
    "                ridgeSGD_training_errors.append(error)\n",
    "                \n",
    "                w = w - (l_rate / m) * (np.dot(X_i.T, error) + lamb * w)\n",
    "                ridgeSGD_duration.append(time() - start)\n",
    "    return ridgeSGD_training_errors, ridgeSGD_training_errors, ridgeSGD_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGDRegressor() of sklearn using penalty as l2\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "stochasticSGD_training_errors = []\n",
    "stochasticSGD_validation_errors = []\n",
    "sklearnSGD_duration = []\n",
    "\n",
    "alphas = np.logspace(-10, 10, 10)\n",
    "def sklearnSGD():\n",
    "    for alpha in alphas:\n",
    "        start = time()\n",
    "        \n",
    "        #SGDRegressor is a standard regularizer for linear SVM models\n",
    "        stochasticGD = SGDRegressor(alpha = alpha, penalty = 'l2', max_iter = 1000)\n",
    "        stochasticGD.fit(X_train, y_train)\n",
    "        \n",
    "        #Predictions using training set\n",
    "        predictions_train_sklearn_sgd = stochasticGD.predict(X_train)\n",
    "        sgd_train_sklearn_mse = mean_squared_error(y_train, predictions_train_sklearn_sgd)\n",
    "        stochasticSGD_training_errors.append(sgd_train_sklearn_mse)\n",
    "        \n",
    "        #Predictions using testing dataset\n",
    "        predictions_sklearn_sgd = stochasticGD.predict(X_test)\n",
    "        sgd_sklearn_mse = mean_squared_error(y_test, predictions_sklearn_sgd)\n",
    "        stochasticSGD_validation_errors.append(sgd_sklearn_mse)\n",
    "        \n",
    "        sklearnSGD_duration.append(time() - start)\n",
    "        #print(f'Using alpha of {alpha}, SGDRegressor has MSE of {sgd_sklearn_mse} with {duration} second execution time.')\n",
    "    return stochasticSGD_training_errors, stochasticSGD_validation_errors, sklearnSGD_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "#Alphas/Lambdas for iterations\n",
    "alphas = np.logspace(-10, 10, 10)\n",
    "\n",
    "sklearnRidge_training_errors = []\n",
    "sklearnRidge_validation_errors = []\n",
    "sklearnRidge_duration = []\n",
    "#Ridge by sklearn\n",
    "#Import Ridge from linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def sklearnRidge():\n",
    "    for alpha in alphas:\n",
    "        start = time() #To check running time \n",
    "        ridge = Ridge(alpha = alpha, normalize = True, tol = 0.01)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        \n",
    "        #Predictions using training set\n",
    "        predictions_train_ridge = ridge.predict(X_train)\n",
    "        train_ridge_mse = mean_squared_error(y_train, predictions_train_ridge)\n",
    "        sklearnRidge_training_errors.append(train_ridge_mse)\n",
    "        \n",
    "        #Predictions using testing set\n",
    "        predictions_ridge = ridge.predict(X_test)\n",
    "        ridge_mse = mean_squared_error(y_test, predictions_ridge)\n",
    "        sklearnRidge_validation_errors.append(ridge_mse)\n",
    "        \n",
    "        sklearnRidge_duration.append(time() - start)\n",
    "        #print(f'Using alpha of {alpha}, Ridge regression has MSE of {ridge_mse} with {duration} second execution time.')\n",
    "    return sklearnRidge_training_errors, sklearnRidge_validation_errors, sklearnRidge_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_sgd(X, y, lamb, max_iter=1000, tol=0.001):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    m = len(y)  #or X.shape[0]\n",
    "    l_rate = 0.01\n",
    "    for i in range(max_iter):\n",
    "        for j in range(X.shape[0]):  #get only some rows\n",
    "            random_row_to_pick = np.random.randint(0, X.shape[0])\n",
    "            X_i = X[random_row_to_pick, :].reshape(1, X_train.shape[1])\n",
    "            y_i = y[[random_row_to_pick]] #shape (n, )\n",
    "            pred = X_i @ w\n",
    "            if (mean_squared_error(y_i, pred) < tol):\n",
    "                break\n",
    "            error = pred - y_i\n",
    "            w = w - (l_rate / m) * (np.dot(X_i.T, error) + lamb * w)\n",
    "    return w\n",
    "\n",
    "def ridge_gd(X, y, lamb, max_iter=1000, tol=0.001):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    m = len(y)  #or X.shape[0]\n",
    "    l_rate = 0.01\n",
    "    for i in range(max_iter):\n",
    "        pred = X @ w\n",
    "        if (mean_squared_error(y, pred) < tol):\n",
    "            break\n",
    "        error = pred - y\n",
    "        w = w - (l_rate / m) * (np.dot(X.T, error) + lamb * w)\n",
    "    return w\n",
    "\n",
    "def apply(X_train, X_test, y_train, y_test, lamb, modifier, use_sklearn, f):\n",
    "    lambs = np.logspace(-5, 3, 8)\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    fittime = []\n",
    "    \n",
    "    for lamb in lambs: \n",
    "        start = time()\n",
    "        if not (use_sklearn):\n",
    "            w = f(X_train, y_train, lamb)\n",
    "            fittime.append(time() - start)\n",
    "            pred = X_train @ w\n",
    "            training_errors.append(mean_squared_error(y_train, pred))\n",
    "            pred = X_test @ w\n",
    "            validation_errors.append(mean_squared_error(y_test, pred))\n",
    "\n",
    "\n",
    "def plot_lambdas_mse(lambs, training_errors, validation_errors, fittime, modifier=\"OLS\"):\n",
    "    _, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax[0].plot(lambs, training_errors, label=f\"{modifier} Training errors\")\n",
    "    ax[0].plot(lambs, validation_errors, label=f\"{modifier} Validation errors\")\n",
    "    ax[0].set_xscale('log')\n",
    "    ax[0].set_xlabel(\"lambdas\")\n",
    "    ax[0].set_ylabel(\"mse\")\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(lambs, fittime, label=f\"{modifier} fittime\")\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_xlabel(\"lambdas\")\n",
    "    ax[1].set_ylabel(\"fittime\")\n",
    "    ax[1].legend()\n",
    "    \n",
    "models = {\"SGD\": ridge_sgd, \"GD\": ridge_gd}\n",
    "use_sklearn = [False, False, False, True, True]\n",
    "\n",
    "for ix, (modifier, model)  in enumerate(models.items()):\n",
    "    apply(X_train, X_test, y_train, y_test, 0.01, modifier, use_sklearn[ix], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaleqs_training_errors, normaleqs_validation_errors, normaleqs_duration = normaleqs_ridge()\n",
    "stochasticSGD_training_errors, stochasticSGD_validation_errors, sklearnSGD_duration = sklearnSGD()\n",
    "sklearnRidge_training_errors, sklearnRidge_validation_errors, sklearnRidge_duration = sklearnRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.logspace(-10, 10, 10)\n",
    "\n",
    "#To plot training errors and validation errors in terms of alpha\n",
    "def plot_compare(training_errors, validation_errors, duration, alpha, title):\n",
    "    _, ax = plt.subplots(1, 2, figsize = (10, 3))\n",
    "    ax[0].plot(alphas, training_errors, label = 'Training Errors')\n",
    "    ax[0].plot(alphas, validation_errors, label = 'Validation Errors')\n",
    "    ax[0].set_xlabel('Lambdas')\n",
    "    ax[0].set_ylabel('Mean Squared Errors')\n",
    "    ax[0].set_xscale('log')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(title)\n",
    "    \n",
    "    ax[1].plot(duration)\n",
    "    #ax[1].set_xscale('log')\n",
    "    #ax[1].set_yscale('log')\n",
    "    ax[1].set_xlabel('Lambda')\n",
    "    ax[1].set_ylabel('Running Time')\n",
    "    plt.title('Fit Time')\n",
    "    plt.tight_layout()\n",
    "plot_compare(normaleqs_training_errors, normaleqs_validation_errors, normaleqs_duration, alpha, 'Closed Form OLS')\n",
    "plot_compare(stochasticSGD_training_errors, stochasticSGD_validation_errors, sklearnSGD_duration, alpha, 'SGD Regressor')\n",
    "plot_compare(sklearnRidge_training_errors, sklearnRidge_validation_errors, sklearnRidge_duration, alpha, 'Ridge by sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note how each algorithms and formulas work within a specific problem. In this example, we are presented with regression models by solving the method analytically (closed form equations) and by solving its optimization algorithms, such as Gradient Descent. As expected, in terms of running time and accuracy, OLS and Ridge (sklearn) provides better accuracy compared to an optimization algorithm. These normal equations have faster convergence than Gradient Descent. This is because Gradient Descent heavily relies with the usage of its alpha. Alpha is the learning step on how it converges to obtain the local minima. It is important to note that when alpha is too large, Gradient Descent can increase the training error. When alpha is small, the algorithm may be stuck with a high training error. The sklearn-Stochastic GD performs faster than Batch GD. Stochastic gradient descent typically reaches convergence much faster than batch  gradient descent since it updates weight more frequently. The SGD computes the gradient of the whole training set and tries to find the max and min from a single traininng dataset. In this way, it generates too much noise. \n",
    "\n",
    "In general, this example shows how important is it to obtain the assumptions of every regression model. Though most algorithms work in these types of problems, assumption testing is important if our objective is to obtain the highest accuracy among these regression problems. Closed-form solutions are much preferred for smaller datasets, since matrices are not a concern. If we want to obtain a regression model that discards multicollinearity and matrix singularity, we can use Gradient Descent(Batch or Stochastic), especially for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
